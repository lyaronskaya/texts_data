{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML MIPT Practical 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном домашнем задании вы будете решать задачу классификации отзывов.\n",
    "\n",
    "Шаги решения:\n",
    "\n",
    "1. Извлечение признаков: напишите код для создания TF-IDF матрицы из представленного корпуса отзывов\n",
    "2. Обучение моделей: напишите код для обучения SVM и логистической регрессии\n",
    "3. Кросс-валидация для подбора гиперпараметров: напишите код для оптимизации метрик обучения\n",
    "4. Участие в контесте на kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "#### Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352278, 2)\n",
      "                                          Reviews_Summary  Prediction\n",
      "ID                                                                   \n",
      "230872                                  Babies love these           3\n",
      "344823                                       Salmon Trout           0\n",
      "211754                                     disappointment           1\n",
      "259421  Doesn't taste like Cinnabon; tastes like Waffl...           2\n",
      "253418  Delicious San Daniele prosciutto and good cust...           3\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.scv', index_col=0, na_values='NaN')\n",
    "print data.shape\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Babies love these', 'Salmon Trout', 'disappointment', ...,\n",
       "        'quite good', 'Great yummy treat for my little ones', 'Disappointed'], dtype=object),\n",
       " array([3, 0, 1, 2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Reviews_Summary.values, data.Prediction.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, каждый объект представляет собой отзыв о продукте и оценку по шкале от 0 до 3. Выдвинем гипотезу, что слова, используемые в написании отзыва коррелируют с оценкой, которая была поставлена. Поставим задачу - предсказать оценку, по тексту отзыва."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### 1. Извлечение признаков - 10 Баллов\n",
    " \n",
    "\n",
    "1. Для решения задачи классификации необходимо преобразовать каждый отзыв (документ) в вектор. Размерность данного вектора будет равна количеству слов используемых в корпусе (все документы). Каждая координата соответствует слову, значение в координает равно количеству раз, слово используется в документе. \n",
    "\n",
    "Для решения данной задачи вам необходимо написать код, который преобразовывает матрицу документов в численную матрицу.\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "\n",
    "- Подробнее про векторное представление документов: http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "- Используйте данный трансформер: http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n",
    "- Подробнее про разреженные матрицы: http://docs.scipy.org/doc/scipy-0.14.0/reference/sparse.html\n",
    "- Hashing trick: https://en.wikipedia.org/wiki/Feature_hashing\n",
    "\n",
    "Используйте n_features = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Babies love these', 'Salmon Trout', 'disappointment', ...,\n",
       "       'quite good', 'Great yummy treat for my little ones', 'Disappointed'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data.Reviews_Summary.values\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, max_features=3000)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352278, 3000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для учета важности редких, но показательных слов (термов), используется схема взвешивания TF-IDF. Напишите код, принимающий на вход разреженную матрицу векторного представления документов и возвращающий разреженную матрицу документов, частоты термов которых взвешенны по TF-IDF.\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "\n",
    "- Подробнее про TF-IDF: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- Используйте трансформер: http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(norm='l2')\n",
    "X = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = data.Prediction.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 2. Код для SVM и логистической регресии - 40 Баллов\n",
    "\n",
    "После того, как вы получили матрицу признаков, вам необходимо реализовать алгоритм обучения SVM и логистической регрессии. Обе модели являются линейными и отличаются функциями потерь. Для решения оптимизационных задач в обеих моделей будет использоваться стохастический градиентный спуск.\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "\n",
    "- Линейные модели: http://cs231n.github.io/linear-classify/\n",
    "- SGD: http://cs231n.github.io/optimization-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с SVM стартовый код находится в файле cs231n/classifiers/linear_svm.py вашей задачей является реализация подсчета функции потерь для SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте обучающую выборку на 2 части train и test\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "- Используйте трансформер: http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транспонируем матрицы с данными, т.к. так будет проще реализовать код SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.T, X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем подвыборки из обучающей выборки, для быстрой проверки кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 100000\n",
    "X_train_sample = X_train[:, :train_size]\n",
    "y_train_sample = y_train[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 100000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем чему равен градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.78 s, sys: 67.8 ms, total: 4.85 s\n",
      "Wall time: 2.32 s\n",
      "loss: 2.999401\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.linear_svm import svm_loss_naive, svm_loss_vectorized\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "%time loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "print 'loss: %f' % (loss, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент равен 0, т.к. код который должен его считать отсутствует. Реализуйте наивную версию и проверьте результат с помощью численного метода расчета. Градиенты должны почти совпадать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000048 analytic: 0.000048, relative error: 8.401037e-08\n",
      "numerical: 0.000031 analytic: 0.000031, relative error: 6.331158e-07\n",
      "numerical: 0.004454 analytic: 0.004454, relative error: 6.072872e-08\n",
      "numerical: -0.000405 analytic: -0.000405, relative error: 7.947509e-08\n",
      "numerical: -0.000024 analytic: -0.000024, relative error: 1.083355e-06\n"
     ]
    }
   ],
   "source": [
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завершите реализацию SVM, реализуйте векторизированную версию расчета градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 2.346205s\n",
      "Vectorized loss and gradient: computed in 0.313998s\n",
      "difference: 0.000011\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print 'Naive loss and gradient: computed in %fs' % (toc - tic)\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'difference: %f' % difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 3.000025\n",
      "iteration 100 / 500: loss 2.604984\n",
      "iteration 200 / 500: loss 2.266889\n",
      "iteration 300 / 500: loss 2.034435\n",
      "iteration 400 / 500: loss 1.891755\n",
      "That took 106.970655s\n",
      "Current loss is 1.783438\n"
     ]
    }
   ],
   "source": [
    "# Now implement SGD in LinearSVM.train() function and run it with the code below\n",
    "from cs231n.classifiers import LinearSVM\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=5e-2, reg=0.01,\n",
    "                      num_iters=500, verbose=True, batch_size=20000)\n",
    "\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)\n",
    "print 'Current loss is %f' % loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function \n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#and evaluate the performance on both the test set\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.23      0.30     11476\n",
      "          1       0.67      0.02      0.04      6605\n",
      "          2       0.42      0.01      0.02     17838\n",
      "          3       0.72      0.98      0.83     80333\n",
      "\n",
      "avg / total       0.64      0.70      0.61    116252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00     11476\n",
      "          1       0.00      0.00      0.00      6605\n",
      "          2       0.00      0.00      0.00     17838\n",
      "          3       0.69      1.00      0.82     80333\n",
      "\n",
      "avg / total       0.48      0.69      0.56    116252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/venv/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print classification_report(y_test, [3]*len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.386788\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000010 analytic: 0.000010, relative error: 4.437077e-07\n",
      "numerical: 0.000041 analytic: 0.000041, relative error: 1.941184e-08\n",
      "numerical: 0.000040 analytic: 0.000040, relative error: 1.147942e-06\n",
      "numerical: -0.000175 analytic: -0.000175, relative error: 8.383948e-08\n",
      "numerical: 0.000004 analytic: 0.000004, relative error: 2.177785e-06\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 1.386800e+00 computed in 4.526186s\n",
      "vectorized loss: 1.386800e+00 computed in 0.231330s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "# print type(loss_vectorized)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.386256\n",
      "iteration 100 / 1000: loss 1.383830\n",
      "iteration 200 / 1000: loss 1.381655\n",
      "iteration 300 / 1000: loss 1.379027\n",
      "iteration 400 / 1000: loss 1.376553\n",
      "iteration 500 / 1000: loss 1.374157\n",
      "iteration 600 / 1000: loss 1.371835\n",
      "iteration 700 / 1000: loss 1.369550\n",
      "iteration 800 / 1000: loss 1.367134\n",
      "iteration 900 / 1000: loss 1.365644\n",
      "That took 23.319193s\n",
      "Current loss is 1.364337\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers import Softmax\n",
    "sm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = sm.train(X_train, y_train, learning_rate=5e-3, reg=0.01,\n",
    "                      num_iters=1000, verbose=True, batch_size=2000)\n",
    "\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)\n",
    "print 'Current loss is %f' % loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x715c612d0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAHuCAYAAADJMutoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUZF3AAzToA/U/6rmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x715c1bed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = sm.predict(X_train)\n",
    "y_test_pred = sm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.20      0.29     15609\n",
      "          1       0.13      0.03      0.05      8811\n",
      "          2       0.25      0.02      0.04     24281\n",
      "          3       0.71      0.97      0.82    109036\n",
      "\n",
      "avg / total       0.59      0.69      0.60    157737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00     15609\n",
      "          1       0.00      0.00      0.00      8811\n",
      "          2       0.00      0.00      0.00     24281\n",
      "          3       0.69      1.00      0.82    109036\n",
      "\n",
      "avg / total       0.48      0.69      0.57    157737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print classification_report(y_test, [3]*len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 3. Kaggle In Class - 50 Баллов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Так как в качестве метрики используется logloss, то будем использовать softmax, так как он почти это выдает в качестве loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-25168340ebbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             loss_hist = sm.train(X_train, y_train, learning_rate=learning_rate, reg=reg,\n\u001b[1;32m----> 8\u001b[1;33m                           num_iters=1000, verbose=False, batch_size=batch_size)\n\u001b[0m\u001b[0;32m      9\u001b[0m             print 'Batch size: %f, reg: %f, learning rate: %f loss: %f' % (batch_size, reg, learning_rate, \n\u001b[0;32m     10\u001b[0m                                                                            np.min(loss_hist[loss_hist > 0]))\n",
      "\u001b[1;32m/home/ubuntu/texts_data/cs231n/classifiers/linear_classifier.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, learning_rate, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mbatch_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "for batch_size in [2**x for x in range(10, 14)]:\n",
    "    for reg in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
    "        for learning_rate in [10, 5, 1, 0.1, 0.01, 0.001]:\n",
    "            sm = Softmax()\n",
    "            loss_hist = sm.train(X_train, y_train, learning_rate=learning_rate, reg=reg,\n",
    "                          num_iters=1000, verbose=False, batch_size=batch_size)\n",
    "            print 'Batch size: %f, reg: %f, learning rate: %f loss: %f' % (batch_size, reg, learning_rate, \n",
    "                                                                           np.min(loss_hist[loss_hist > 0]))\n",
    "                   \n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "for batch_size in [2**x for x in range(11, 14)]:\n",
    "    for reg in [1, 0.1, 0.01, 0.05, 0.001, 0.005, 0.0001]:\n",
    "        for learning_rate in [30, 20, 10, 5, 1]:\n",
    "            sm = Softmax()\n",
    "            loss_hist = sm.train(X_train, y_train, learning_rate=learning_rate, reg=reg,\n",
    "                          num_iters=1000, verbose=False, batch_size=batch_size)\n",
    "            print 'Batch size: %f, reg: %f, learning rate: %f loss: %f' % (batch_size, reg, learning_rate, \n",
    "                                                                           np.min(loss_hist[loss_hist > 0]))\n",
    "                   \n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 4. Бонусы - 30 Баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используем stop_words='english' и $l_2$ нормализацию в TF_IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = data.Reviews_Summary.values\n",
    "vectorizer = CountVectorizer(min_df=1, max_features=3500, stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "transformer = TfidfTransformer(norm='l2')\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train, X_test = X_train.T, X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1.386268\n",
      "iteration 100 / 1000: loss 1.386268\n",
      "iteration 200 / 1000: loss 1.386268\n",
      "iteration 300 / 1000: loss 1.386268\n",
      "iteration 400 / 1000: loss 1.386268\n",
      "iteration 500 / 1000: loss 1.386268\n",
      "iteration 600 / 1000: loss 1.386268\n",
      "iteration 700 / 1000: loss 1.386268\n",
      "iteration 800 / 1000: loss 1.386268\n",
      "iteration 900 / 1000: loss 1.386268\n",
      "That took 252.451672s\n",
      "Current loss is -391.906731\n"
     ]
    }
   ],
   "source": [
    "sm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = sm.train(X_train, y_train, learning_rate=7, reg=1,\n",
    "                      num_iters=1000, verbose=True, batch_size=20000)\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)\n",
    "print 'Current loss is %f' % loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = sm.predict_prob(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [     4 116252]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-72815526ee89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu/anaconda2/envs/venv/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     \u001b[1;31m# Check if dimensions are consistent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1560\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1561\u001b[0m     \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/envs/venv/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[1;32m--> 176\u001b[1;33m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [     4 116252]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function log_loss in module sklearn.metrics.classification:\n",
      "\n",
      "log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None)\n",
      "    Log loss, aka logistic loss or cross-entropy loss.\n",
      "    \n",
      "    This is the loss function used in (multinomial) logistic regression\n",
      "    and extensions of it such as neural networks, defined as the negative\n",
      "    log-likelihood of the true labels given a probabilistic classifier's\n",
      "    predictions. For a single sample with true label yt in {0,1} and\n",
      "    estimated probability yp that yt = 1, the log loss is\n",
      "    \n",
      "        -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
      "    \n",
      "    Read more in the :ref:`User Guide <log_loss>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like or label indicator matrix\n",
      "        Ground truth (correct) labels for n_samples samples.\n",
      "    \n",
      "    y_pred : array-like of float, shape = (n_samples, n_classes)\n",
      "        Predicted probabilities, as returned by a classifier's\n",
      "        predict_proba method.\n",
      "    \n",
      "    eps : float\n",
      "        Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "        clipped to max(eps, min(1 - eps, p)).\n",
      "    \n",
      "    normalize : bool, optional (default=True)\n",
      "        If true, return the mean loss per sample.\n",
      "        Otherwise, return the sum of the per-sample losses.\n",
      "    \n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    loss : float\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n",
      "    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "    0.21616...\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "    p. 209.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The logarithm used is the natural logarithm (base-e).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('texts_data/test.csv', index_col=0, na_values='NaN')\n",
    "corpus_test = test_df[\"Reviews_Summary\"].values\n",
    "test_vectorized = vectorizer.transform(corpus_test)\n",
    "test_tfidf = transformer.transform(test_vectorized).toarray()\n",
    "pred = sm.predict_prob(test_array.T)\n",
    "results_matrix = pred.T\n",
    "result = pd.DataFrame(test_df[\"Id\"])\n",
    "\n",
    "ans.index = ids_test\n",
    "ans.index.name=\"Id\"\n",
    "ans.columns=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"]\n",
    "ans.to_csv('kaggle_data/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
